import os, yaml

def get_stack():

    stack = '''#!/usr/bin/env python3

# This stack was generated by "gen_cdk_stack" - modifications might be overwritten

import os, yaml
import aws_cdk as core
from aws_cdk import (
    Duration,
    Stack,
    Size,
    CfnOutput,
    aws_iam as iam,
    aws_lambda as _lambda,
    aws_s3 as s3,
    aws_s3_notifications as s3_notifications,
    aws_events as events,
    aws_events_targets as targets,
 )
from constructs import Construct

class Stack(Stack):
    def create_lambda(self, name, env, lambda_role):
        # Intially we will load dummy code, to prevent dependencies=related failueres
        # Later in the create-stack script we will load the full/real code as zip
        dummy_code  = "def events_handler(events, context):\\n"
        dummy_code += '    msg = \\'ERROR: dummy code for Lambda "NAME" - please run "update_lambdas"\\'\\n'
        dummy_code += "    print(msg)\\n"
        dummy_code += "    return { 'statusCode': 400, 'message': msg }\\n"
        return _lambda.Function(
            self,
            name,
            runtime=_lambda.Runtime.PYTHON_3_13,
            code=_lambda.InlineCode(dummy_code.replace("NAME", name)),
            handler="index.events_handler",
            role=lambda_role,
            memory_size=10240,
            ephemeral_storage_size=Size. gibibytes(10),
            timeout=core.Duration.minutes(15),
            environment=env
        )
    def get_bucket_name(self, idx, name_type):
        match name_type:
            case "short":
                return f"bucket-{self.bucket_names[idx]}"
            case "real":
                return f"{self.stack_name}--bucket-{self.bucket_names[idx].lower()}"
            case "real_for_env_var":
                return self.get_bucket_name(idx, "real").upper().replace("-", "_")
            case "short_for_env_var":
                return self.get_bucket_name(idx, "short").upper().replace("-", "_")
            case _:
                raise "ERROR: illigal name_type"

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)


        # 1. Create buckets
        self.bucket_names = ^^bucket_names^^
        preexisting_buckets = ^^preexisting_buckets^^
        buckets = {}
        for idx in range(len(self.bucket_names)):
            bucket_short_name = self.get_bucket_name(idx, "short")
            bucket_real_name = self.get_bucket_name(idx, "real")

            # 1.a Create the bucket
            if bucket_short_name in preexisting_buckets:
                buckets[bucket_short_name] = s3.Bucket.from_bucket_name(self, bucket_short_name, bucket_real_name)
            else: 
                buckets[bucket_short_name] = s3.Bucket(
                    self, bucket_short_name,
                    bucket_name=bucket_real_name,
                    removal_policy=core.RemovalPolicy.DESTROY,
                    #auto_delete_objects=True
                )
                # 1.b Add a statment to the IAM policy to deny non-HTTPS requests:
                buckets[bucket_short_name].add_to_resource_policy(
                    iam.PolicyStatement(
                        sid="DenyHTTP",
                        effect=iam.Effect.DENY,
                        # This blocks everyone if aws:SecureTransport is false
                        principals=[iam.AnyPrincipal()],
                        actions=["s3:*"],
                        resources=[buckets[bucket_short_name].bucket_arn, f"{buckets[bucket_short_name].bucket_arn}/*"],
                        conditions={ "Bool": {"aws:SecureTransport": "false"}}
                    )
                )
            
        # 2. Create Lambdas
            self.lambda_names = ^^lambda_names^^
            lambdas = {}
            for idx in range(len(self.lambda_names)):

                # 2.a Create lambda role
                lambda_role = iam.Role(self,f"LambdaRole{idx}", assumed_by=iam.ServicePrincipal("lambda.amazonaws.com"))
                # Allow basic Lambda logging
                lambda_role.add_managed_policy(
                    iam.ManagedPolicy.from_aws_managed_policy_name("service-role/AWSLambdaBasicExecutionRole")
                )
                
                # 2.b Create lambda envirnment
                env= {}
                # env["BUCKET"] = self.get_bucket_name(idx, "real_for_env_var")
                
                # 2.c Create lambda envirnment
                lambda_name = f"{self.lambda_names[idx]}"
                lambda_code_file_name = f"{lambda_name}_lambda"
                lambdas[lambda_name] = self.create_lambda(lambda_code_file_name, env, lambda_role)


        # 3. Set Lambdas access rights to buckets
        # 3.a Permission to read only this bucket
        read_only_lambda_access_to_bucket = ^^read_only_lambda_access_to_bucket^^
        for bucket_name in read_only_lambda_access_to_bucket.keys():
            for lambda_name in read_only_lambda_access_to_bucket[bucket_name]:
                #buckets[bucket_name].grant_read(lambdas[lambda_name])
                lambdas[lambda_name].role.add_to_policy(
                    iam.PolicyStatement(
                        actions=["s3:GetObject", "s3:ListBucket"],
                        resources=[bucket.bucket_arn, f"{bucket.bucket_arn}/*"]
                    )
                )
        # 3.b Permission to read and write on this bucket
        read_write_lambda_access_to_bucket = ^^read_write_lambda_access_to_bucket^^
        for bucket_name in read_write_lambda_access_to_bucket.keys():
            for lambda_name in read_write_lambda_access_to_bucket[bucket_name]:
                #buckets[bucket_name].grant_read_write(lambdas[lambda_name])
                lambdas[lambda_name].role.add_to_policy(
                    iam.PolicyStatement(
                        actions=["s3:GetObject", "s3:PutObject", "s3:DeleteObject", "s3:ListBucket"],
                        resources=[buckets['bucket-'+bucket_name].bucket_arn, f"{buckets['bucket-'+bucket_name].bucket_arn}/*"]
                    )
                )
        # 5. Define Lambda-running triggers
        trigger_on_file_drop = ^^trigger_on_file_drop^^
        for bucket_name in trigger_on_file_drop.keys():
            for lambda_name in trigger_on_file_drop[bucket_name]:
                notification = s3_notifications.LambdaDestination(lambdas[lambda_name])
                buckets["bucket-"+bucket_name].add_event_notification(
                    s3.EventType.OBJECT_CREATED, notification,
                    #s3.NotificationKeyFilter(suffix=".yaml")
                )
 

        # 6. Send the names of all Lambdas to the output, to be used to upload the lambdas
        for file_name in lambdas.keys():
           function_name = lambdas[file_name].function_name
           env_var_name = file_name.upper().replace("-", "_") + "_FUNCTION_NAME" 
           # Example: converts aaa_bbb_ccc to aaaBbbCccFunctionName
           #env_var_name = "".join([ w.capitalize() for w in file_name.split("_") ])
           #env_var_name = env_var_name[0].lower() + env_var_name[1:] +"FunctionName"
           CfnOutput(self, env_var_name, value=function_name)

app = core.App()
Stack(app, "^^stack_name^^")
app.synth()
'''


    # 1. Read the parameters from the config file
    if not os.path.isfile("config.yaml"):
        raise "ERROR: the file 'config.yaml' not found"
    config = yaml.safe_load(open("config.yaml"))
    config_params  = [ "stack_name", "lambda_names", "bucket_names" ]
    for config_param in config_params:
        locals()[config_param] = config[config_param]

    # 2. embded parameters in the stack
    preexisting_buckets = []
    read_only_lambda_access_to_bucket = {}
    read_write_lambda_access_to_bucket = {}
    trigger_on_file_drop = {}
    for bn in config["bucket_properties"].keys():
         props = config["bucket_properties"][bn]
         for prop in props:
             if prop == ("pre-existing"): 
                 preexisting_buckets.append(bn)
             if prop.startswith("Grand read-only access to the lambda"):
                 if not bn in read_only_lambda_access_to_bucket.keys():
                     read_only_lambda_access_to_bucket[bn] = [ ]
                 lambda_name = prop.split()[-1]             
                 read_only_lambda_access_to_bucket[bn].append(lambda_name)                
             if prop.startswith("Grand read-write access to the lambda"):
                 if not bn in read_write_lambda_access_to_bucket.keys():
                     read_write_lambda_access_to_bucket[bn] = [ ]
                 lambda_name = prop.split()[-1]               
                 read_write_lambda_access_to_bucket[bn].append(lambda_name)                
             if prop.startswith("On file drop trigger the lambda"):
                 if not bn in trigger_on_file_drop.keys():
                     trigger_on_file_drop[bn] = [ ]
                 lambda_name = prop.split()[-1]               
                 trigger_on_file_drop[bn].append(lambda_name)                

    config_params  = [ "stack_name", "lambda_names", "bucket_names" ]
    config_params += ["preexisting_buckets"]
    config_params += ["read_only_lambda_access_to_bucket"]
    config_params += ["read_write_lambda_access_to_bucket"]
    config_params += ["trigger_on_file_drop"]
    for param in config_params:
         stack = stack.replace("^^"+param+"^^", str(locals()[param]))
    return stack

if __name__=="__main__":
    print(get_stack())
